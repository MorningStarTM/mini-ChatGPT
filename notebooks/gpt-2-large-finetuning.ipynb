{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opendatasets datasets trl","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:43:43.206038Z","iopub.execute_input":"2024-09-08T13:43:43.206439Z","iopub.status.idle":"2024-09-08T13:43:59.336666Z","shell.execute_reply.started":"2024-09-08T13:43:43.206400Z","shell.execute_reply":"2024-09-08T13:43:59.335451Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting opendatasets\n  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nCollecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from opendatasets) (4.66.4)\nRequirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (from opendatasets) (1.6.17)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from opendatasets) (8.1.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.4.0)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.44.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.33.0)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle->opendatasets) (1.16.0)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle->opendatasets) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle->opendatasets) (8.0.4)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle->opendatasets) (6.1.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle->opendatasets) (0.5.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nDownloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\nDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, tyro, opendatasets, trl\nSuccessfully installed opendatasets-0.1.22 shtab-1.7.1 trl-0.10.1 tyro-0.8.10\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    TrainingArguments,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    pipeline,\n    logging,\n)\nfrom trl import SFTTrainer\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:43:59.338615Z","iopub.execute_input":"2024-09-08T13:43:59.338947Z","iopub.status.idle":"2024-09-08T13:44:19.964115Z","shell.execute_reply.started":"2024-09-08T13:43:59.338914Z","shell.execute_reply":"2024-09-08T13:44:19.963329Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"batch_size = 4\nnum_workers = os.cpu_count()\nmax_steps = 6000\nbf16 = False\nfp16 = True\ngradient_accumulation_steps = 2\ncontext_length = 512\nlogging_steps = 500\nsave_steps = 500\nlearning_rate = 0.0001\nmodel_name = 'openai-community/gpt2-medium'\nout_dir = 'outputs/gpt2-finetuned'","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:19.965184Z","iopub.execute_input":"2024-09-08T13:44:19.965777Z","iopub.status.idle":"2024-09-08T13:44:19.971285Z","shell.execute_reply.started":"2024-09-08T13:44:19.965743Z","shell.execute_reply":"2024-09-08T13:44:19.970249Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if bf16:\n    model = AutoModelForCausalLM.from_pretrained(model_name).to(dtype=torch.bfloat16)\nelse:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nprint(model)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:19.973787Z","iopub.execute_input":"2024-09-08T13:44:19.974155Z","iopub.status.idle":"2024-09-08T13:44:26.968589Z","shell.execute_reply.started":"2024-09-08T13:44:19.974113Z","shell.execute_reply":"2024-09-08T13:44:26.967630Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc3037d89924eb989e3c3c9989218de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61959e4ca8134233a6fc1e01b6be39f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15789fd365bc4ae181c90213db2b0c7c"}},"metadata":{}},{"name":"stdout","text":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n)\n354,823,168 total parameters.\n354,823,168 training parameters.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    use_fast=False\n)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:26.969797Z","iopub.execute_input":"2024-09-08T13:44:26.970166Z","iopub.status.idle":"2024-09-08T13:44:31.233483Z","shell.execute_reply.started":"2024-09-08T13:44:26.970101Z","shell.execute_reply":"2024-09-08T13:44:31.232647Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050fc261e2ed473b82de1a5a7bc79f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"178db2cad44d473c880d50dd2860e143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a44c256f7e2547ddbbbb9a89b02d7e7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec5f83c966a409eba7c1ae1d65e30e8"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset('JM-Lee/Phi-3-mini-128k-instruct_instruction')\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:31.234693Z","iopub.execute_input":"2024-09-08T13:44:31.235004Z","iopub.status.idle":"2024-09-08T13:44:40.661918Z","shell.execute_reply.started":"2024-09-08T13:44:31.234972Z","shell.execute_reply":"2024-09-08T13:44:40.660951Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/359 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17538b30b97544b9bf9a5d2bdc260279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/68.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd1f074c1c240b4bd275cb7f5a9565b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/61135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792ac84bfc094ceb8abfd1c3490d39a4"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['system', 'instruction', 'response'],\n        num_rows: 61135\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"full_dataset = dataset['train'].train_test_split(test_size=0.05, shuffle=True)\ndataset_train = full_dataset['train']\ndataset_valid = full_dataset['test']\n\nprint(dataset_train)\nprint(dataset_valid)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:40.663053Z","iopub.execute_input":"2024-09-08T13:44:40.663370Z","iopub.status.idle":"2024-09-08T13:44:40.823593Z","shell.execute_reply.started":"2024-09-08T13:44:40.663337Z","shell.execute_reply":"2024-09-08T13:44:40.822646Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['system', 'instruction', 'response'],\n    num_rows: 58078\n})\nDataset({\n    features: ['system', 'instruction', 'response'],\n    num_rows: 3057\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(example):\n    \"\"\"\n    Formatting function returning a list of samples (kind of necessary for SFT API).\n    \"\"\"\n    text = f\"### Instruction:\\n{example['system']}\\n\\n### Input:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:40.824658Z","iopub.execute_input":"2024-09-08T13:44:40.824993Z","iopub.status.idle":"2024-09-08T13:44:40.882019Z","shell.execute_reply.started":"2024-09-08T13:44:40.824959Z","shell.execute_reply":"2024-09-08T13:44:40.880871Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f\"{out_dir}/logs\",\n    evaluation_strategy='steps',\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    logging_strategy='steps',\n    save_strategy='steps',\n    logging_steps=logging_steps,\n    save_steps=save_steps,\n    save_total_limit=2,\n    bf16=bf16,\n    fp16=fp16,\n    report_to='tensorboard',\n    max_steps=max_steps,\n    dataloader_num_workers=num_workers,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    learning_rate=learning_rate,\n    lr_scheduler_type='constant',\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:40.883322Z","iopub.execute_input":"2024-09-08T13:44:40.883628Z","iopub.status.idle":"2024-09-08T13:44:40.962487Z","shell.execute_reply.started":"2024-09-08T13:44:40.883595Z","shell.execute_reply":"2024-09-08T13:44:40.961663Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset_train,\n    eval_dataset=dataset_valid,\n    max_seq_length=context_length,\n    tokenizer=tokenizer,\n    args=training_args,\n    formatting_func=preprocess_function,\n    packing=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:44:40.964888Z","iopub.execute_input":"2024-09-08T13:44:40.965206Z","iopub.status.idle":"2024-09-08T13:50:03.963288Z","shell.execute_reply.started":"2024-09-08T13:44:40.965173Z","shell.execute_reply":"2024-09-08T13:50:03.962346Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af0dd6c311841cb8c422f2e51f44e98"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1399 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"064a578f0aad4ca9a9b1bd51ff9fa295"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Sample","metadata":{}},{"cell_type":"code","source":"dataloader = trainer.get_train_dataloader()\nfor i, sample in enumerate(dataloader):\n    print(tokenizer.decode(sample['input_ids'][0]))\n    print('#'*50)\n    if i == 5:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:50:03.964520Z","iopub.execute_input":"2024-09-08T13:50:03.964822Z","iopub.status.idle":"2024-09-08T13:50:04.242112Z","shell.execute_reply.started":"2024-09-08T13:50:03.964789Z","shell.execute_reply":"2024-09-08T13:50:04.240915Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":" = \\sum_{i=0}^{2} nums[i] = 12$.\n\\end{document}\n```\n\nHowever, this approach is not recommended because it's not practical to compute the sum directly in LaTeX. It's better to calculate the sum in your programming language and then typeset the result in LaTeX.<|endoftext|>### Instruction:\n\nYou are helpful and informative ai assistant.\n\n<Constitutions>\n1. You are a helpful and informative AI assistant.\n2. You should not respond with false information.\n3. When you solve the problem, you should think step by step.\n\nMake sure you follow the rules.\n\n### Input:\ninstruction:\nIn this task, you are given a sentence in the English language and your task is to convert it into the Japanese language. In translation, keep numbers as it is and make it sentence case (capitalize only the first word of each sentence and noun).\nquestion:\nBefore the act, he posted a suicide note on his website.\nanswer:\n自殺する前に、彼はウェブサイトに遺書を公表した。\n\n\nquestion:\nThe train was heading from the northeastern town of Bijelo Polje to the Montenegrin coastal city of Bar.\nanswer:\n列車は北東部の町であるBijeloPoljeからモンテネグロ沿岸都市であるBarへ向かっていた。\n\n\nquestion:\nThe state's governor Dannel Malloy signed the repeal into law on Wednesday and is to take effect immediately.\nanswer:\n\n\n### Response:\n<|start_page_id|>1<|end_page_id|>\n<|start_context_id|>English to Japanese Translation<|end_context_id|>\n<|start_context_info|>\n<|context_info|>\n<|context_text|>The state's governor Dannel Malloy signed the repeal into law on Wednesday and is to take effect immediately.<|end_context_text|>\n<|end_context_info|>\n<|start_translation_id|>Translation<|end_translation_id|>\n<|start\n##################################################\n### Input:\nTASK DEFINITION: A text is given in Bengali. Translate it from the Bengali language to the Malayalam language. The translation must not omit or add information to the original sentence.\nPROBLEM: প্রধানমন্ত্রীর সঙ্গে আলাপচারিতার জন্য সারা দেশ জুড়ে ৬০০টিরও বেশি কেন্দ্রের প্রত্যেকটিতে তিন জন করে উপভোক্তা উপস্থিত ছিলেন।\n\nSOLUTION: രാജ്യത്തുടനീളമുള്ള ഉജ്ജ്വല ഗുണഭോക്താക്കളുമായി പ്രധാനമന്ത്രി ശ്രീ. നര�\n##################################################\n photos.\n\n<|start_step_id|>Step 3: Organize Your Images<|end_step_id|>\n\nOnce you have your images, it's time to organize them effectively. Here are some tips to help you:\n\n- Group similar images together. For example, group images related to your career, relationships, or personal growth.\n- Use a color scheme to create a cohesive look. For example, use a monochromatic color scheme or complementary colors.\n- Arrange your images in a way that tells a story. For example, start with images that represent your current situation and end with images that represent your future goals.\n\n<|start_step_id|>Step 4: Create Your Vision Board<|end_step_id|>\n\nNow that you have your images organized, it's time to create your vision board. Here are some tips to help you:\n\n- Choose a format that works for you. You can create a physical collage or a digital vision board.\n- Use a large, sturdy surface to create your vision board. You can use a corkboard, a bulletin board, or a canvas.\n- Use adhesive materials like push pins, tape, or glue to attach your images.\n- Add text and labels to your vision board to make it more personal and meaningful.\n\n<|start_step_id|>Step 5: Stay Motivated and Reflect on Your Progress<|end_step_id|>\n\nCreating a vision board is just the beginning. To stay motivated and make progress towards your goals, it's essential to reflect on your progress regularly. Here are some tips to help you:\n\n- Set specific, measurable, achievable, relevant, and time-bound (SMART) goals.\n- Track your progress regularly and celebrate your achievements.\n- Reflect on your vision board regularly and update it as needed.\n- Surround yourself with positive influences and supportive people.\n\n<|start_step_id|>Step 6: Explore Different Styles of Vision Boards<|end_step_id|>\n\nFinally, let's explore different styles of vision boards. Here are some suggestions:\n\n- Collage: A physical collage is a classic and traditional way to create a vision board. It's a great way to use physical images and textures.\n- Digital: A digital vision board is a modern and convenient way to create a vision board\n##################################################\nNow, solve for R:\n\nR = 100 / 30\nR = 10 / 3\nR = 3.333...%\n\nTo express this as a mixed fraction, we have:\n\nR = 3 1/3%\n\nHowever, this is not one of the options provided. Let's double-check our calculation.\n\nWe know that for the money to double, the interest earned should be equal to the principal. So, the interest earned (SI) is equal to the principal (P).\n\nSI = P\n\nUsing the simple interest formula:\n\nP = P × R × T / 100\n\nSince SI = P, we can write:\n\nP = P × R × 30 / 100\n\nNow, we can cancel out P from both sides:\n\n1 = R × 30 / 100\n\nNow, solve for R:\n\nR = 100 / 30\nR = 10 / 3\nR = 3.333...%\n\nTo express this as a mixed fraction, we have:\n\nR = 3 1/3%\n\nHowever, this is not one of the options provided. It seems there might be a mistake in the options given. The correct answer should be 3 1/3%, but since it's not listed, we'll choose the closest option, which is:\n\n(A) 3 1/9%\n\nThis is the closest to the correct answer of 3 1/3%, but it's important to note that there seems to be an error in the options provided.<|endoftext|>### Instruction:\n\nYou are helpful and informative ai assistant.\n\n<Constitutions>\n1. You are a helpful and informative AI assistant.\n2. You should not respond with false information.\n3. When you solve the problem, you should think step by step.\n\nMake sure you follow the rules.\n\n### Input:\nInstructions: Indicate a text span in the passage about which an inquisitive question is asked. (note, don't answer the question).\nInput: Sentence: Mr. Mosbacher called for concrete results by next spring in negotiations over fundamental Japanese business practices that supposedly inhibit free trade.<sep>Question: What are Japanese businesses doing that inhibit free trade?\nOutput:\n\n### Response:\nWhat are Japanese businesses doing that inhibit free trade?\n<|start_span_id|>Japanese businesses doing that inhibit free trade<\n##################################################\nDear Camila,\n\nIt's wonderful to hear about your rich and diverse experience in the field of dance and your passion for helping others through the power of dance, movement, and music. Your background in Psychology, Dance Therapy, and your entrepreneurial spirit are all valuable assets that can be leveraged in various career paths.\n\n1. **Career Path Options:**\n\n   - **Dance Therapist/Dance/Movement Therapist:** Given your background in Dance Therapy and your passion for helping people, you could consider pursuing a certification in Dance/Movement Therapy. This would allow you to work in various settings such as hospitals, schools, and community centers, helping individuals improve their physical, emotional, and mental well-being through dance.\n\n   - **Educator/Teacher:** With your extensive experience in teaching dance to various age groups, you could consider becoming a dance instructor or a dance teacher in schools, community centers, or private studios. You could also consider teaching dance therapy or movement therapy courses.\n\n   - **Counselor/Therapist:** Your background in Psychology and Dance Therapy could be a great fit for a career in counseling or therapy. You could work in a mental health clinic, hospital, or private practice, helping individuals improve their mental and emotional well-being.\n\n   - **Entrepreneur/Event Organizer:** Your entrepreneurial spirit and experience in running a dance studio and organizing events could be utilized in starting your own dance therapy or movement therapy business. You could also consider organizing workshops, cultural events, concerts, and festivals that incorporate dance and movement therapy.\n\n2. **Resume Building:**\n\n   - Highlight your diverse experience in teaching dance, your background in Psychology, and your Dance Therapy internship.\n\n   - Emphasize your entrepreneurial skills and experience in running a dance studio and organizing events.\n\n   - Include any relevant certifications, courses, or workshops you have completed.\n\n   - Showcase your passion for helping others through dance and movement therapy.\n\n3. **Job Resources:**\n\n   - **American Dance Therapy Association (ADTA):** This organization offers resources for dance therapists, including job listings, continuing education, and networking opportunities.\n\n   - **International Association for Dance Medicine & Science (IADMS):** This organization provides resources for dance professionals, including job listings, research, and networking opportunities.\n\n   - **LinkedIn:** Use LinkedIn\n##################################################\n python function?\n\ndef cal\\_score\\_age\\_gender(img1, img1\\_filename, img2, img2\\_filename, facial\\_similarity, predict\\_age\\_and\\_gender,\n detect\\_face):\n # Similarity check\n response = {}\n technical\\_details = []\n img1 = np.asarray(Image.open(img1.stream).convert('RGB'))\n img2 = np.asarray(Image.open(img2.stream).convert('RGB'))\n\n can\\_process\\_res = can\\_process\\_image(img1, img2)\n\n if facial\\_similarity:\n if can\\_process\\_res['can\\_process']==False:\n if can\\_process\\_res['problem\\_with\\_image1'] and can\\_process\\_res['problem\\_with\\_image2']:\n similarity = \"Couldn't predict similarity due to low image quality of {0} and {1}\".format(img1\\_filename,\n img2\\_filename)\n elif can\\_process\\_res['problem\\_with\\_image1']:\n similarity = \"Couldn't predict similarity due to low image quality of {0}\".format(img1\\_filename)\n else:\n similarity = \"Couldn't predict similarity due to low image quality of {0}\".format(img2\\_filename)\n else:\n\n res = DeepFace.verify(img1, img2, detector\\_backend=DETECTOR\\_BACKEND\\_1, prog\\_bar=False,\n model\\_name=MODEL\\_NAME, model=MODEL, distance\\_metric=METRIC)\n\n score = round(res['distance'], 2)\n\n similarity = distance\\_2\\_category(score, THRESHOLD[0],THRESHOLD[1])\n\n response['similarity'] = similarity\n\n technical\\_details.append(\"Similarity score: '{0}'\".format(score))\n if detect\\_face | predict\\_age\\_and\\_gender:\n response['image-1'] = {'filename': img1\\_filename}\n response['image-2'] = {'filename': img\n##################################################\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"history = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:50:04.243729Z","iopub.execute_input":"2024-09-08T13:50:04.244069Z","iopub.status.idle":"2024-09-08T17:33:27.694084Z","shell.execute_reply.started":"2024-09-08T13:50:04.244036Z","shell.execute_reply":"2024-09-08T17:33:27.692999Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6000/6000 3:43:19, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.715300</td>\n      <td>1.586389</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.636400</td>\n      <td>1.548085</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.605400</td>\n      <td>1.511550</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.557400</td>\n      <td>1.487412</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.537400</td>\n      <td>1.472860</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.549300</td>\n      <td>1.459531</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.512900</td>\n      <td>1.449006</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.509000</td>\n      <td>1.435389</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.496500</td>\n      <td>1.428405</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.489000</td>\n      <td>1.421715</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.496600</td>\n      <td>1.413167</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.484500</td>\n      <td>1.404510</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe = pipeline(\n    task='text-generation',\n    model=model,\n    tokenizer=tokenizer,\n    max_length=512, # Prompt + new tokens to generate.\n    device_map=device\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T17:33:56.225196Z","iopub.execute_input":"2024-09-08T17:33:56.225622Z","iopub.status.idle":"2024-09-08T17:33:56.233376Z","shell.execute_reply.started":"2024-09-08T17:33:56.225583Z","shell.execute_reply":"2024-09-08T17:33:56.232431Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"template = \"\"\"### Instruction:\n{}\n### Input:\n{}\n### Response:\n{}\"\"\"\n\ninstructions = 'Write three tips to learn computer science.'\ninputs = ''\nresponse = ''\nprompt = template.format(instructions, inputs, response)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T17:34:43.144626Z","iopub.execute_input":"2024-09-08T17:34:43.145474Z","iopub.status.idle":"2024-09-08T17:34:43.150993Z","shell.execute_reply.started":"2024-09-08T17:34:43.145426Z","shell.execute_reply":"2024-09-08T17:34:43.149878Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"outputs = pipe(\n    prompt,\n    do_sample=True,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n    repetition_penalty=1.1,\n)\nprint(outputs[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T17:35:17.064419Z","iopub.execute_input":"2024-09-08T17:35:17.065078Z","iopub.status.idle":"2024-09-08T17:35:20.862592Z","shell.execute_reply.started":"2024-09-08T17:35:17.065039Z","shell.execute_reply":"2024-09-08T17:35:20.861587Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"### Instruction:\nWrite three tips to learn computer science.\n### Input:\n\n### Response:\n1. **Learn Python Programming Language**: Python is a free, open-source, and widely used programming language that can be used for web development, web application development, and more. It's a powerful tool for beginners, as it's easy to understand and versatile for different projects.\n\n2. **Learn Java Programming Language**: Java is a popular choice for developing web applications, but it's also known as the \"language of the browser\" due to its simplicity and ease of use.\n   - Learn Java syntax, syntax error handling, and object-oriented concepts.\n\n3. **Learn JavaScript Programming Language**: JavaScript is a JavaScript-based scripting language that can be used in various programming languages. It's known for its simplicity and efficiency, making it a great choice for beginners.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}